<p align="center">
  <img src="docs/logo_v2.png" alt="AnyBimanual Logo">
</p>

### [**AnyBimanual: Transferring Single-arm Policy for General Bimanual Manipulation**](#)  
[Guanxing Lu <sup>*</sup>](https://guanxinglu.github.io/), [Tengbo Yu<sup>*</sup>](https://github.com/TengBoYuu?tab=repositories), [Haoyuan Deng](https://github.com/Denghaoyuan123?tab=repositories), [Season Si Chen](https://www.sigs.tsinghua.edu.cn/Chensi_en/main.htm), [Ziwei Wang](https://ziweiwangthu.github.io/), [Yansong Tang <sup>â€ </sup>](https://andytang15.github.io/)

**[[Project Page](https://anybimanual.github.io/)] | [[Paper](#)]**

![](docs/pipeline.png)


ğŸ‰ **NEWS**: 

- *Dec. 2024:* Codebase is released!.

# ğŸ“ TODO
- [ ] Release pretrained checkpoints.

# ğŸ’» Installation

**NOTE**: AnyBimanual is mainly built upon the [Perceiver-Actor^2](https://github.com/markusgrotz/peract_bimanual) repo by Markus Grotz et al.

See [INSTALL.md](docs/INSTALLATION.md) for installation instructions. 

See [ERROR_CATCH.md](docs/ERROR_CATCH.md) for error catching.

# ğŸ› ï¸ Usage

The following steps are structured in order.

## ğŸ—ƒï¸ Generate Demonstrations 

Please checkout the website for [pre-generated RLBench
demonstrations](https://bimanual.github.io). If you directly use these
datasets, you don't need to run `tools/bimanual_data_generator.py` from
RLBench. Using these datasets will also help reproducibility since each scene
is randomly sampled in `data_generator_bimanual.py`.


## ğŸš† Training
We use wandb to log some curves and visualizations. Login to wandb before running the scripts.
```bash
wandb login
```
To train our PerAct + AnyBimanual, run:
```bash
bash scripts/train.sh BIMANUAL_PERACT 0,1 12345 ${exp_name}
```
where the `exp_name` can be specified as you like.

To train our PerAct-LF + AnyBimanual, run:
```bash
bash scripts/train.sh PERACT_BC 0,1 12345 ${exp_name}
```

To train our RVT-LF + AnyBimanual, run:
```bash
bash scripts/train.sh RVT 0,1 12345 ${exp_name}
```

Set the `augmentation_type` in the `scripts/train.sh` to choose whether to apply the augmentation methods mentioned in our paper or to use the original SE3 augmentation.

## ğŸ”¬ Evaluation
To evaluate the checkpoint in simulator, you can use:
```bash
bash scripts/eval.sh BIMANUAL_PERACT 0 ${exp_name}
```

# ğŸ¦¾ Real Robot

### ğŸ® Prepare data in real world

[Demonstrations Collection by teleoperation](https://github.com/Denghaoyuan123/Bimanual_ur5e_joystick_control)

Data convert into RLbench2 form
```bash
python3 anybimanual_real_supply/data/preprocess_ntu_dualarm.py
```  
Keyframe selection
```bash
python3 anybimanual_real_supply/data/auto_keyframe_mani.py
```

### ğŸ¯ Finetune
```bash
bash scripts/train_real.sh BIMANUAL_PERACT 0,1 12345 ${exp_name}
```

### ğŸ•¹ï¸ Evaluation on real robot
Run model inference scripts to receive real-world observation to generate actions, here we give an example of the Agent Class.
```bash
python3 anybimanual_real_supply/eval_agent_on_robot.py
```

After receiving the action generated by the model, you can refer to [Bimanual_ur5e_action_control_for_IL](https://github.com/Denghaoyuan123/Bimanual_ur5e_action_control_for_IL) to drive dual_UR5e to perform the action.

# ğŸ·ï¸ License
This repository is released under the MIT license.

# ğŸ™ Acknowledgement

Our code is built upon [Perceiver-Actor^2](https://github.com/markusgrotz/peract_bimanual), [SkillDiffuser](https://github.com/Liang-ZX/skilldiffuser), [PerAct](https://github.com/peract/peract), [RLBench](https://github.com/stepjam/RLBench), and [CLIP](https://github.com/openai/CLIP). We thank all these authors for their nicely open sourced code and their great contributions to the community.

# ğŸ”— Citation
If you find this repository helpful, please consider citing:

```
@article{lu2024anybimanual,
  author    = {Lu, Guanxing and Yu, Tengbo and Deng, Haoyuan and Chen, Season Si and Wang, Ziwei and Tang, Yansong},
  title     = {AnyBimanual: Transferring Single-arm Policy for General Bimanual Manipulation},
  year      = {2024},
}
```
